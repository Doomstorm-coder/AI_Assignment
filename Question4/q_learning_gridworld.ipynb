{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fe03a30",
   "metadata": {},
   "source": [
    "# Q-Learning in GridWorld\n",
    "\n",
    "This project simulates a **5x5 GridWorld** where an agent learns the optimal policy using **Q-learning**. Special transitions (teleportation) and rewards are assigned to specific cells, encouraging the agent to learn effective behavior through exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04dbe68",
   "metadata": {},
   "source": [
    "## Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98287b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tkinter as tk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6fa16c",
   "metadata": {},
   "source": [
    "### Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ed9df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID_SIZE = 5\n",
    "A, A_PRIME = (0, 1), (4, 1)\n",
    "B, B_PRIME = (0, 3), (2, 3)\n",
    "A_REWARD, B_REWARD = 10, 5\n",
    "ACTIONS = ['N', 'S', 'E', 'W']\n",
    "ACTION_DELTAS = {'N': (-1, 0), 'S': (1, 0), 'E': (0, 1), 'W': (0, -1)}\n",
    "ARROWS = {'N': '↑', 'S': '↓', 'E': '→', 'W': '←'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857d1e50",
   "metadata": {},
   "source": [
    "## Environment Transition Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249193e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(state, action):\n",
    "    if state == A:\n",
    "        return A_PRIME, A_REWARD\n",
    "    elif state == B:\n",
    "        return B_PRIME, B_REWARD\n",
    "\n",
    "    delta = ACTION_DELTAS[action]\n",
    "    new_row, new_col = state[0] + delta[0], state[1] + delta[1]\n",
    "    if 0 <= new_row < GRID_SIZE and 0 <= new_col < GRID_SIZE:\n",
    "        return (new_row, new_col), 0\n",
    "    else:\n",
    "        return state, -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5e4649",
   "metadata": {},
   "source": [
    "## Action Selection: Epsilon-Greedy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6667297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(Q, state):\n",
    "    if random.random() < EPSILON:\n",
    "        return random.choice(ACTIONS)\n",
    "    else:\n",
    "        max_q = max(Q[state].values())\n",
    "        return random.choice([a for a in ACTIONS if Q[state][a] == max_q])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc67995",
   "metadata": {},
   "source": [
    "## Q-Table Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fbc742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_Q():\n",
    "    return { (i, j): {a: 0.0 for a in ACTIONS} for i in range(GRID_SIZE) for j in range(GRID_SIZE) }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08437b3c",
   "metadata": {},
   "source": [
    "## Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2338e39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.9\n",
    "EPSILON = 0.1\n",
    "ALPHA = 0.1\n",
    "EPISODES = 500\n",
    "STEPS_PER_EPISODE = 10\n",
    "\n",
    "def q_learning():\n",
    "    print(\"Initializing Gridworld...\")\n",
    "    print(f\"Grid size: {GRID_SIZE}x{GRID_SIZE}\")\n",
    "    print(f\"Special_states = {{'A': {A}, 'B': {B}}}\")\n",
    "    print(f\"Next_to_states = {{'A\\'': {A_PRIME}, 'B\\'': {B_PRIME}}}\")\n",
    "    print(f\"Special_rewards = {{'A': {A_REWARD}, 'B': {B_REWARD}}}\")\n",
    "    print(\"Starting Q-learning with parameters:\")\n",
    "    print(f\" γ = {GAMMA}\")\n",
    "    print(f\" ε = {EPSILON}\")\n",
    "    print(f\" α = {ALPHA}\")\n",
    "    print(f\" Episodes = {EPISODES}\")\n",
    "    print(f\"Steps = {EPISODES * STEPS_PER_EPISODE}\")\n",
    "\n",
    "    Q = initialize_Q()\n",
    "\n",
    "    for _ in range(EPISODES):\n",
    "        state = (random.randint(0, GRID_SIZE - 1), random.randint(0, GRID_SIZE - 1))\n",
    "        for _ in range(STEPS_PER_EPISODE):\n",
    "            action = epsilon_greedy(Q, state)\n",
    "            next_state, reward = step(state, action)\n",
    "            max_next = max(Q[next_state].values())\n",
    "            Q[state][action] += ALPHA * (reward + GAMMA * max_next - Q[state][action])\n",
    "            state = next_state\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99a91e0",
   "metadata": {},
   "source": [
    "## Extracting Value Function & Optimal Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb689f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_value_and_policy(Q):\n",
    "    V = np.zeros((GRID_SIZE, GRID_SIZE))\n",
    "    policy = np.full((GRID_SIZE, GRID_SIZE), '', dtype=object)\n",
    "    for i in range(GRID_SIZE):\n",
    "        for j in range(GRID_SIZE):\n",
    "            state = (i, j)\n",
    "            best_action = max(Q[state], key=Q[state].get)\n",
    "            V[i, j] = Q[state][best_action]\n",
    "            policy[i, j] = ARROWS[best_action]\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3f0b52",
   "metadata": {},
   "source": [
    "## Result Printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1dcd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(V, policy):\n",
    "    print(\"Evaluating optimal value function and policy...\")\n",
    "    print(\"Optimal Value Function:\")\n",
    "    for row in V:\n",
    "        print(\" \".join(f\"{val:5.2f}\" for val in row))\n",
    "    print(\"Optimal Policy (arrows):\")\n",
    "    for row in policy:\n",
    "        print(\" \".join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265bd964",
   "metadata": {},
   "source": [
    "## GUI Display with Tkinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac4351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_gui(V, policy):\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Q-Learning GridWorld Visualization\")\n",
    "\n",
    "    cell_size = 80\n",
    "    canvas = tk.Canvas(root, width=GRID_SIZE * cell_size, height=GRID_SIZE * cell_size)\n",
    "    canvas.pack()\n",
    "\n",
    "    for i in range(GRID_SIZE):\n",
    "        for j in range(GRID_SIZE):\n",
    "            x1, y1 = j * cell_size, i * cell_size\n",
    "            x2, y2 = x1 + cell_size, y1 + cell_size\n",
    "            canvas.create_rectangle(x1, y1, x2, y2, fill=\"white\", outline=\"black\")\n",
    "\n",
    "            value = V[i][j]\n",
    "            canvas.create_text(x1 + cell_size / 2, y1 + 20, text=f\"{value:.2f}\", font=(\"Helvetica\", 12, \"bold\"))\n",
    "            arrow = policy[i][j]\n",
    "            canvas.create_text(x1 + cell_size / 2, y1 + 50, text=arrow, font=(\"Helvetica\", 20))\n",
    "\n",
    "    root.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce698d50",
   "metadata": {},
   "source": [
    "## Run Q-learning and Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d57b4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = q_learning()\n",
    "V, policy = extract_value_and_policy(Q)\n",
    "print_results(V, policy)\n",
    "# Uncomment to show GUI\n",
    "# display_gui(V, policy)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}